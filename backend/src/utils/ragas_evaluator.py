"""
Evaluador RAGAS para medir la calidad de las respuestas del agente.
Captura datos durante la ejecuci√≥n y calcula m√©tricas de evaluaci√≥n.
"""
import logging
import os
from typing import Dict, Any, List, Optional
from dataclasses import dataclass, field
from datetime import datetime

logger = logging.getLogger(__name__)

# Import opcional de ragas - si no est√° disponible, el evaluador funcionar√° en modo degradado
try:
    from ragas import evaluate
    from ragas.metrics import (
        faithfulness,
        answer_relevancy,
        context_precision,
        context_recall
    )
    from datasets import Dataset
    try:
        from langchain_openai import ChatOpenAI
        LANGCHAIN_OPENAI_AVAILABLE = True
    except ImportError:
        # Fallback: intentar importar desde langchain.llms o langchain.chat_models
        try:
            from langchain.chat_models import ChatOpenAI
            LANGCHAIN_OPENAI_AVAILABLE = True
        except ImportError:
            LANGCHAIN_OPENAI_AVAILABLE = False
            ChatOpenAI = None
    # Intentar importar LangchainLLMWrapper de RAGAS para envolver el LLM
    try:
        from ragas.llms import LangchainLLMWrapper
        RAGAS_LLM_WRAPPER_AVAILABLE = True
    except ImportError:
        RAGAS_LLM_WRAPPER_AVAILABLE = False
        LangchainLLMWrapper = None
    import pandas as pd
    import numpy as np
    RAGAS_AVAILABLE = True
except ImportError as e:
    RAGAS_AVAILABLE = False
    pd = None
    np = None
    LANGCHAIN_OPENAI_AVAILABLE = False
    ChatOpenAI = None
    RAGAS_LLM_WRAPPER_AVAILABLE = False
    LangchainLLMWrapper = None
    logger.warning(f"Ragas o dependencias no est√°n instaladas: {e}. El evaluador funcionar√° en modo degradado (solo captura de datos).")


@dataclass
class EvaluationData:
    """Estructura de datos para una evaluaci√≥n individual"""
    question: str
    answer: str
    contexts: List[str] = field(default_factory=list)
    ground_truth: Optional[str] = None
    metadata: Dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=datetime.now)


class RAGASEvaluator:
    """
    Evaluador que captura datos durante la ejecuci√≥n del agente
    y calcula m√©tricas RAGAS para evaluar la calidad de las respuestas.
    """
    
    def __init__(self, enabled: bool = True):
        """
        Inicializa el evaluador.
        
        Args:
            enabled: Si est√° deshabilitado, no captura datos ni calcula m√©tricas
        """
        self.enabled = enabled and RAGAS_AVAILABLE
        self.evaluation_data: List[EvaluationData] = []
        self.metrics_history: List[Dict[str, Any]] = []
        
        if not RAGAS_AVAILABLE and enabled:
            logger.warning(
                "Ragas no est√° disponible. El evaluador solo capturar√° datos "
                "pero no calcular√° m√©tricas. Instala con: pip install ragas datasets"
            )
    
    def capture_evaluation(
        self,
        question: str,
        answer: str,
        contexts: Optional[List[str]] = None,
        ground_truth: Optional[str] = None,
        metadata: Optional[Dict[str, Any]] = None
    ):
        """
        Captura datos de una evaluaci√≥n individual.
        
        Args:
            question: Pregunta del usuario
            answer: Respuesta generada por el agente
            contexts: Contextos utilizados para generar la respuesta
            ground_truth: Respuesta esperada (opcional, para evaluaci√≥n)
            metadata: Metadatos adicionales (herramienta usada, tiempo, etc.)
        """
        if not self.enabled:
            return
        
        eval_data = EvaluationData(
            question=question,
            answer=answer,
            contexts=contexts or [],
            ground_truth=ground_truth,
            metadata=metadata or {}
        )
        
        self.evaluation_data.append(eval_data)
        logger.info(f"[RAGAS] üíæ Evaluaci√≥n capturada: {question[:50]}... (contextos: {len(contexts or [])})")
        
        # OPTIMIZACI√ìN: Limitar el tama√±o de evaluation_data para evitar problemas de memoria
        MAX_EVALUATION_DATA = 50  # Mantener solo las √∫ltimas 50 evaluaciones
        if len(self.evaluation_data) > MAX_EVALUATION_DATA:
            # Eliminar las evaluaciones m√°s antiguas
            self.evaluation_data = self.evaluation_data[-MAX_EVALUATION_DATA:]
            logger.debug(f"[RAGAS] Limpiadas evaluaciones antiguas, manteniendo {MAX_EVALUATION_DATA} m√°s recientes")
    
    def evaluate_batch(
        self,
        questions: List[str],
        answers: List[str],
        contexts: List[List[str]],
        ground_truths: Optional[List[str]] = None
    ) -> Dict[str, float]:
        """
        Eval√∫a un lote de preguntas y respuestas usando m√©tricas RAGAS.
        
        Args:
            questions: Lista de preguntas
            answers: Lista de respuestas generadas
            contexts: Lista de listas de contextos usados para cada respuesta
            ground_truths: Lista opcional de respuestas esperadas
        
        Returns:
            Diccionario con las m√©tricas calculadas
        """
        if not self.enabled or not RAGAS_AVAILABLE:
            logger.warning("Ragas no est√° disponible. No se pueden calcular m√©tricas.")
            return {}
        
        if len(questions) != len(answers) or len(questions) != len(contexts):
            raise ValueError("Las listas de questions, answers y contexts deben tener la misma longitud")
        
        try:
            # Configurar API key de OpenAI para RAGAS
            # RAGAS internamente crea un cliente OpenAI, as√≠ que necesitamos
            # asegurarnos de que la variable de entorno est√© configurada
            from ..settings import settings
            
            # Guardar el valor actual si existe
            original_api_key = os.environ.get("OPENAI_API_KEY")
            
            # Configurar la API key desde settings
            if settings.openai_api_key:
                os.environ["OPENAI_API_KEY"] = settings.openai_api_key
                logger.debug("[RAGAS] API key de OpenAI configurada desde settings")
            else:
                logger.warning("[RAGAS] ‚ö†Ô∏è No se encontr√≥ openai_api_key en settings")
            
            # Preparar dataset para Ragas
            data_dict = {
                "question": questions,
                "answer": answers,
                "contexts": contexts
            }
            
            # Agregar ground_truth si est√° disponible
            if ground_truths:
                if len(ground_truths) != len(questions):
                    raise ValueError("ground_truths debe tener la misma longitud que questions")
                data_dict["ground_truth"] = ground_truths
            
            dataset = Dataset.from_dict(data_dict)
            
            # Configurar LLM de LangChain para RAGAS en lugar de InstructorLLM por defecto
            # Esto resuelve el problema de compatibilidad con agenerate_prompt
            ragas_llm = None
            if RAGAS_AVAILABLE and LANGCHAIN_OPENAI_AVAILABLE and ChatOpenAI is not None:
                try:
                    from ..settings import settings
                    # Crear el LLM de LangChain
                    langchain_llm = ChatOpenAI(
                        model=settings.llm_model,
                        api_key=settings.openai_api_key,
                        temperature=0
                    )
                    # Envolver el LLM con LangchainLLMWrapper si est√° disponible
                    if RAGAS_LLM_WRAPPER_AVAILABLE and LangchainLLMWrapper is not None:
                        ragas_llm = LangchainLLMWrapper(langchain_llm)
                        logger.debug("[RAGAS] LLM de LangChain envuelto con LangchainLLMWrapper para RAGAS")
                    else:
                        # Si no hay wrapper, usar el LLM directamente (puede funcionar en algunas versiones)
                        ragas_llm = langchain_llm
                        logger.debug("[RAGAS] LLM de LangChain configurado para RAGAS (sin wrapper)")
                except Exception as llm_error:
                    logger.warning(f"[RAGAS] ‚ö†Ô∏è Error al configurar LLM para RAGAS: {llm_error}. Usando configuraci√≥n por defecto.")
                    ragas_llm = None
            else:
                logger.debug("[RAGAS] ChatOpenAI no disponible, usando configuraci√≥n por defecto de RAGAS")
            
            # Definir m√©tricas a calcular
            # Las m√©tricas de RAGAS son objetos/clases, no funciones
            # El LLM se configura a nivel de evaluate() o mediante configuraci√≥n global
            metrics = [
                faithfulness,      # Mide si la respuesta es fiel al contexto
                answer_relevancy  # Mide si la respuesta es relevante para la pregunta
            ]
            
            # M√©tricas que S√ç requieren ground truth (reference):
            # context_precision y context_recall requieren la columna 'reference'
            if ground_truths:
                # Agregar m√©tricas que requieren ground truth
                metrics.append(context_precision)  # Requiere 'reference'
                metrics.append(context_recall)     # Requiere 'reference'
            
            # Calcular m√©tricas de forma as√≠ncrona para evitar BlockingError
            logger.info(f"Evaluando {len(questions)} casos con RAGAS...")
            result = None
            eval_error_occurred = False
            try:
                # Configurar el LLM para RAGAS
                # En algunas versiones de RAGAS, el LLM se puede pasar a evaluate()
                # o configurar mediante variables de entorno
                evaluate_kwargs = {
                    "dataset": dataset,
                    "metrics": metrics
                }
                
                # Si tenemos un LLM configurado, intentar pasarlo a evaluate()
                # RAGAS puede aceptar el LLM de diferentes formas seg√∫n la versi√≥n
                if ragas_llm:
                    # Intentar diferentes formas de pasar el LLM seg√∫n la versi√≥n de RAGAS
                    # Algunas versiones aceptan 'llm', otras 'generator_llm', otras lo configuran globalmente
                    try:
                        # M√©todo 1: Intentar pasar como 'llm'
                        evaluate_kwargs["llm"] = ragas_llm
                        logger.debug("[RAGAS] Intentando pasar LLM como par√°metro 'llm'")
                    except (TypeError, KeyError):
                        try:
                            # M√©todo 2: Intentar pasar como 'generator_llm'
                            evaluate_kwargs["generator_llm"] = ragas_llm
                            logger.debug("[RAGAS] Intentando pasar LLM como par√°metro 'generator_llm'")
                        except (TypeError, KeyError):
                            # M√©todo 3: Configurar globalmente (si RAGAS lo soporta)
                            logger.debug("[RAGAS] No se pudo pasar LLM como par√°metro, RAGAS usar√° configuraci√≥n por defecto")
                
                # Ejecutar RAGAS directamente
                # Nota: Si hay BlockingError, se puede ejecutar con --allow-blocking o BG_JOB_ISOLATED_LOOPS=true
                result = evaluate(**evaluate_kwargs)
            except Exception as eval_error:
                # Capturar errores durante la evaluaci√≥n (pueden ser errores internos de RAGAS)
                error_msg = str(eval_error)
                eval_error_occurred = True
                if "agenerate_prompt" in error_msg or "InstructorLLM" in error_msg:
                    logger.warning(
                        f"[RAGAS] ‚ö†Ô∏è Error de compatibilidad en RAGAS (posible problema de versi√≥n): {error_msg}. "
                        "Los errores internos pueden impedir el c√°lculo de m√©tricas."
                    )
                    # Aunque hay error, RAGAS a veces retorna un resultado parcial
                    # Intentar continuar para ver si hay algo √∫til
                    logger.info("[RAGAS] Intentando continuar a pesar de los errores internos...")
                else:
                    # Re-lanzar otros errores
                    logger.error(f"[RAGAS] Error inesperado durante evaluaci√≥n: {error_msg}")
                    raise
            
            # Si no hay resultado y hubo error, retornar vac√≠o
            if result is None and eval_error_occurred:
                logger.warning("[RAGAS] ‚ö†Ô∏è No se obtuvo resultado de RAGAS debido a errores internos")
                return {}
            
            # Convertir resultado a diccionario
            # RAGAS puede retornar diferentes tipos de resultados
            metrics_dict = {}
            
            try:
                # Log del tipo de resultado para debugging
                logger.debug(f"[RAGAS] Tipo de resultado: {type(result)}")
                
                # Intentar acceder como diccionario/objeto Dataset
                if hasattr(result, 'to_pandas') and pd is not None:
                    # Si es un Dataset, convertir a pandas y luego a dict
                    df = result.to_pandas()
                    logger.debug(f"[RAGAS] DataFrame shape: {df.shape}, columnas: {list(df.columns)}")
                    
                    # Columnas que NO son m√©tricas (datos de entrada)
                    non_metric_columns = ['question', 'answer', 'contexts', 'ground_truth', 'reference']
                    # Obtener la media de cada columna de m√©tricas (solo num√©ricas)
                    for col in df.columns:
                        if col not in non_metric_columns:
                            try:
                                # Intentar convertir a num√©rico y calcular media
                                numeric_col = pd.to_numeric(df[col], errors='coerce')
                                if not numeric_col.isna().all():  # Si hay al menos un valor num√©rico
                                    mean_value = numeric_col.mean()
                                    if pd.notna(mean_value):
                                        metrics_dict[col] = float(mean_value)
                                        logger.debug(f"[RAGAS] ‚úÖ M√©trica '{col}': {mean_value:.4f}")
                            except (ValueError, TypeError) as e:
                                # Si no se puede convertir a num√©rico, ignorar esta columna
                                logger.debug(f"[RAGAS] Columna '{col}' no es num√©rica, omitiendo: {e}")
                                continue
                    
                    # Si no encontramos m√©tricas, intentar buscar columnas que contengan nombres de m√©tricas conocidas
                    if not metrics_dict:
                        known_metrics = ['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall']
                        for metric_name in known_metrics:
                            # Buscar columnas que contengan el nombre de la m√©trica
                            matching_cols = [col for col in df.columns if metric_name.lower() in col.lower()]
                            for col in matching_cols:
                                try:
                                    numeric_col = pd.to_numeric(df[col], errors='coerce')
                                    if not numeric_col.isna().all():
                                        mean_value = numeric_col.mean()
                                        if pd.notna(mean_value):
                                            metrics_dict[metric_name] = float(mean_value)
                                            logger.debug(f"[RAGAS] ‚úÖ M√©trica encontrada '{metric_name}' en columna '{col}': {mean_value:.4f}")
                                except:
                                    continue
                elif hasattr(result, '__iter__') and not isinstance(result, (str, bytes)):
                    # Intentar iterar sobre las claves
                    try:
                        # Si tiene m√©todo keys() o es un dict-like
                        if hasattr(result, 'keys'):
                            for metric_name in result.keys():
                                value = result[metric_name]
                                if isinstance(value, (int, float)):
                                    metrics_dict[metric_name] = float(value)
                                elif hasattr(value, 'mean'):
                                    metrics_dict[metric_name] = float(value.mean())
                                elif hasattr(value, '__iter__') and not isinstance(value, (str, bytes)):
                                    # Si es una lista/serie, calcular media
                                    try:
                                        if np is not None:
                                            metrics_dict[metric_name] = float(np.mean(value))
                                        else:
                                            metrics_dict[metric_name] = float(sum(value) / len(value))
                                    except:
                                        metrics_dict[metric_name] = float(sum(value) / len(value)) if value else 0.0
                        else:
                            # Intentar acceder directamente a las m√©tricas conocidas
                            for metric in metrics:
                                metric_name = getattr(metric, '__name__', str(metric))
                                if hasattr(result, metric_name):
                                    value = getattr(result, metric_name)
                                    if isinstance(value, (int, float)):
                                        metrics_dict[metric_name] = float(value)
                                    elif hasattr(value, 'mean'):
                                        metrics_dict[metric_name] = float(value.mean())
                    except Exception as e:
                        logger.warning(f"[RAGAS] Error al acceder a resultados: {e}. Tipo de resultado: {type(result)}")
                        # Si todo falla, intentar convertir a dict directamente
                        if hasattr(result, '__dict__'):
                            metrics_dict = {k: float(v) if isinstance(v, (int, float)) else 0.0 
                                          for k, v in result.__dict__.items() 
                                          if isinstance(v, (int, float))}
                else:
                    logger.warning(f"[RAGAS] Formato de resultado no reconocido: {type(result)}")
            except Exception as e:
                logger.error(f"[RAGAS] Error al procesar resultados: {e}", exc_info=True)
                # Si hay errores pero el resultado tiene alg√∫n valor, intentar extraerlo
                if hasattr(result, '__dict__'):
                    metrics_dict = {k: float(v) if isinstance(v, (int, float)) else 0.0 
                                  for k, v in result.__dict__.items() 
                                  if isinstance(v, (int, float))}
            
            # Guardar en historial solo si hay m√©tricas
            if metrics_dict:
                self.metrics_history.append({
                    "timestamp": datetime.now().isoformat(),
                    "num_cases": len(questions),
                    "metrics": metrics_dict
                })
                logger.info(f"[RAGAS] ‚úÖ M√©tricas calculadas: {metrics_dict}")
            else:
                logger.warning("[RAGAS] ‚ö†Ô∏è No se pudieron extraer m√©tricas del resultado")
                # Logging detallado para diagn√≥stico
                if result is not None:
                    logger.debug(f"[RAGAS] Tipo de resultado: {type(result)}")
                    if hasattr(result, '__dict__'):
                        logger.debug(f"[RAGAS] Atributos del resultado: {list(result.__dict__.keys())}")
                    if hasattr(result, 'to_pandas') and pd is not None:
                        try:
                            df = result.to_pandas()
                            logger.info(f"[RAGAS] üîç DataFrame shape: {df.shape}")
                            logger.info(f"[RAGAS] üîç Columnas disponibles: {list(df.columns)}")
                            logger.debug(f"[RAGAS] Primeras filas del DataFrame:\n{df.head()}")
                            # Intentar mostrar tipos de datos
                            logger.debug(f"[RAGAS] Tipos de datos:\n{df.dtypes}")
                        except Exception as e:
                            logger.debug(f"[RAGAS] Error al convertir a pandas para debugging: {e}")
                else:
                    logger.warning("[RAGAS] ‚ö†Ô∏è El resultado de RAGAS es None - posible fallo completo de la evaluaci√≥n")
            
            return metrics_dict
            
        except Exception as e:
            logger.error(f"Error al evaluar con RAGAS: {str(e)}", exc_info=True)
            return {}
        finally:
            # Restaurar el valor original de OPENAI_API_KEY si exist√≠a
            if original_api_key is not None:
                os.environ["OPENAI_API_KEY"] = original_api_key
            elif "OPENAI_API_KEY" in os.environ and not settings.openai_api_key:
                # Si no hab√≠a valor original y settings no tiene key, eliminar la variable
                del os.environ["OPENAI_API_KEY"]
    
    def evaluate_captured_data(self) -> Dict[str, float]:
        """
        Eval√∫a todos los datos capturados previamente.
        
        Returns:
            Diccionario con las m√©tricas calculadas
        """
        if not self.evaluation_data:
            logger.warning("No hay datos capturados para evaluar")
            return {}
        
        questions = [d.question for d in self.evaluation_data]
        answers = [d.answer for d in self.evaluation_data]
        contexts = [d.contexts for d in self.evaluation_data]
        # Solo incluir ground_truths si TODOS los datos tienen ground_truth
        # Si algunos tienen y otros no, RAGAS no puede evaluar correctamente
        ground_truths_list = [d.ground_truth for d in self.evaluation_data]
        ground_truths = ground_truths_list if all(gt is not None and gt.strip() for gt in ground_truths_list) else None
        
        if ground_truths is None and any(gt is not None for gt in ground_truths_list):
            logger.warning("[RAGAS] Algunos datos tienen ground_truth y otros no. Se omitir√° ground_truth para esta evaluaci√≥n.")
        
        return self.evaluate_batch(questions, answers, contexts, ground_truths)
    
    def clear_data(self):
        """Limpia todos los datos capturados"""
        self.evaluation_data.clear()
        logger.info("Datos de evaluaci√≥n limpiados")
    
    def get_summary(self) -> Dict[str, Any]:
        """
        Obtiene un resumen de las evaluaciones realizadas.
        
        Returns:
            Diccionario con estad√≠sticas y m√©tricas promedio
        """
        summary = {
            "total_evaluations": len(self.evaluation_data),
            "total_metrics_runs": len(self.metrics_history),
            "metrics_history": self.metrics_history
        }
        
        if self.metrics_history:
            # Calcular promedios de todas las m√©tricas hist√≥ricas
            all_metrics = {}
            for run in self.metrics_history:
                for metric_name, value in run.get("metrics", {}).items():
                    if metric_name not in all_metrics:
                        all_metrics[metric_name] = []
                    all_metrics[metric_name].append(value)
            
            summary["average_metrics"] = {
                metric_name: sum(values) / len(values)
                for metric_name, values in all_metrics.items()
            }
        
        return summary


# Instancia global del evaluador (opcional, puede ser deshabilitado)
_global_evaluator: Optional[RAGASEvaluator] = None


def get_evaluator(enabled: bool = True) -> RAGASEvaluator:
    """
    Obtiene la instancia global del evaluador.
    
    Args:
        enabled: Si debe estar habilitado (solo afecta la primera creaci√≥n)
    
    Returns:
        Instancia del evaluador
    """
    global _global_evaluator
    if _global_evaluator is None:
        _global_evaluator = RAGASEvaluator(enabled=enabled)
    return _global_evaluator


def reset_evaluator():
    """Reinicia el evaluador global (√∫til para tests)"""
    global _global_evaluator
    _global_evaluator = None

